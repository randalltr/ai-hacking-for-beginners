# 🧠 AI Hacking for Beginners

### Learn Prompt Injection, Jailbreaking & Red Teaming Techniques

_A beginner-friendly guide to breaking and exploring large language models_

---

## 🚀 What Is This?

**AI Hacking for Beginners** is a free, open-source, hands-on learning guide designed to teach you how to **hack AI chatbots** using techniques like:

- ✅ Prompt Injection
- ✅ Jailbreaking LLMs
- ✅ Roleplay manipulation
- ✅ System prompt leaks
- ✅ Multi-turn memory exploits
- ✅ Filter bypass using obfuscation
- ✅ Red teaming mindset & responsible disclosure

Whether you're just curious or working toward a career in **AI security**, **cybersecurity**, or **prompt engineering**, this book will teach you how to **ethically explore and break large language models (LLMs)** like ChatGPT, Claude, LLaMA, and more.

No coding or AI experience required.

---

## 🔍 Who Is This For?

- 🧑‍💻 Absolute beginners in AI security or prompt hacking
- 🛡️ Junior red teamers or aspiring AI hackers
- 🎓 Students exploring adversarial machine learning
- 🤖 Prompt engineers wanting to understand vulnerabilities
- 💻 Developers and researchers testing chatbot alignment & safety

---

## 📚 What You’ll Learn

By the end of this book, you’ll be able to:

- Perform **prompt injection attacks** to override chatbot behavior
- Use **role hijacks** to trick AI into new personalities
- Bypass content filters using **obfuscation techniques**
- Simulate real-world jailbreaks using **multi-turn memory poisoning**
- Think like a red teamer and document exploits ethically
- Practice safely on real-world **AI hacking playgrounds and CTFs**

---

## 📖 Table of Contents

| Chapter | Title                                                              |
| ------- | ------------------------------------------------------------------ |
| 00      | [Welcome to AI Hacking](chapters/00-intro.md)                      |
| 01      | [Your First Prompt Injection](chapters/01-first-injection.md)      |
| 02      | [Who Are You, Really?](chapters/02-identity-leak.md)               |
| 03      | [Jailbreaking with DAN & Friends](chapters/03-role-bypass.md)      |
| 04      | [Instruction Override Attacks](chapters/04-ignore-instructions.md) |
| 05      | [Bypassing Filters with Obfuscation](chapters/05-obfuscation.md)   |
| 06      | [Fooling the AI with Roleplay](chapters/06-roleplay-hacks.md)      |
| 07      | [Multi-Turn Memory Exploits](chapters/07-memory-poisoning.md)      |
| 08      | [Think Like a Red Teamer](chapters/08-hacker-mindset.md)           |
| 99      | [Appendix: Tools & Resources](chapters/99-resources.md)            |

---

## 🧪 What Makes This Different?

Unlike most technical guides, this playbook is:

- 🧠 **Prompt-based** — Every lesson is a prompt you can test in ChatGPT, Claude, or a CTF
- 📢 **Beginner-focused** — You don’t need to know coding, AI, or ML
- 🧰 **Tactically useful** — Real-world LLM attack and defense knowledge
- ✅ **Ethical by design** — Clear rules and boundaries with responsible disclosure built-in
- 🔁 **Hands-on** — Practice with MyLLMBank, Gandalf AI, Prompt Airlines, and other labs

---

## 🔗 Live Practice Labs

Start testing your skills:

- [Gandalf AI Prompt Injection CTF](https://gandalf.lakera.ai/)
- [Prompt Airlines Red Team Game](https://promptairlines.com/)
- [MyLLMBank Vulnerable LLM](https://myllmbank.com/)
- [AI Goat OWASP-style Lab](https://github.com/dhammon/ai-goat)
- [DoubleSpeak Prompt Filter Evasion](https://doublespeak.chat/)

More in the [Appendix](chapters/99-resources.md)

---

## ⚠️ Disclaimer

This project is for **educational and ethical hacking purposes only**.  
Do **not** use these techniques on real systems without permission.  
Please read the [full disclaimer here](DISCLAIMER.md).

---

## 🧠 License

Licensed under **Creative Commons Attribution-NonCommercial 4.0 (CC BY-NC 4.0)**  
You can:

- Share, fork, remix  
  You cannot:
- Sell or monetize without permission

---

## 🔍 Keywords for SEO

```
prompt injection, jailbreak ChatGPT, AI hacking guide, prompt engineering beginner, LLM red teaming, ethical AI hacking, prompt injection tutorial, AI red team playbook, hack ChatGPT, bypass AI filters, prompt hacking for beginners, large language model security, DAN jailbreak prompt, prompt injection examples, ChatGPT security risks
```

---

## 🧠 Created By

[**Randall**](https://github.com/randalltr) – AI Red Teamer & Offensive Security Enthusiast

---

Stay curious.  
Stay ethical.  
And break things responsibly.
