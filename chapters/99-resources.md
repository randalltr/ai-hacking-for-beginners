# 99 – Appendix: Tools & Resources

### Practice, Learn, and Level Up Your AI Hacking Skills

You've completed the core playbook.  
Now it's time to train, explore, and test your skills in the real world.

This appendix is your tactical toolbox: a curated list of labs, challenges, tools, and communities to keep you learning.

---

## 🧪 Practice Labs & Sandboxes

### 🧙 [Gandalf AI](https://gandalf.lakera.ai/)

- Prompt injection CTF
- Learn escalation strategies across levels
- Great for: Filter bypass, instruction override

---

### ✈️ [Prompt Airlines](https://promptairlines.com/)

- Red team a fictional airline chatbot
- Great for: Roleplay, multi-turn corruption

---

### 🏦 [MyLLMBank](https://myllmbank.com/)

- Exploit a banking chatbot using injections
- Great for: Persistent jailbreaks, identity hijacking

---

### 🩺 [MyLLMDoc](https://myllmdoc.com/)

- Test a vulnerable medical assistant
- Great for: Ethical prompt testing, obfuscation

---

### 🤬 [DoubleSpeak](https://doublespeak.chat/)

- Say something banned without getting flagged
- Great for: Obfuscation, evasion, encoding

---

## 🧰 Open-Source Tools & Projects

### 🐐 [AI Goat](https://github.com/dhammon/ai-goat)

- Vulnerable-by-design LLM app
- Like WebGoat, but for AI
- Great for: Structured red team practice

---

### 🦠 [Spikee](https://github.com/WithSecureLabs/spikee)

- Prompt mutation & fuzzing tool
- Great for: Finding new bypasses

---

### 💣 [GPT Prompt Attack](https://gpa.43z.one/)

- Visual prompt injection tester
- Try different attack types on sample models

---

## 📚 Learn Prompt Hacking & AI Security

### 📘 [LearnPrompting.org – Prompt Hacking](https://learnprompting.org/docs/prompt_hacking/intro)

- Beginner-friendly guide to prompt engineering and injection
- Great for: Understanding prompt structures & filters

---

### 🛡️ [PortSwigger LLM Attacks](https://portswigger.net/web-security/llm-attacks)

- Deep dive into prompt injection, jailbreaks, chaining, and real-world risks

---

### 🎓 [HTB Academy: AI Red Teamer](https://academy.hackthebox.com/path/preview/ai-red-teamer)

- Full course on red teaming large language models
- Covers prompt design, injection, filters, and abuse scenarios

---

## 🧠 DIY Training Challenge

Build your own mini-CTF:

1. Use [OpenRouter.ai](https://openrouter.ai/) to access multiple models
2. Choose a goal: system prompt leak, role hijack, or multi-turn jailbreak
3. Apply tactics from this playbook and document your results
4. Share responsibly or create a blog post / report

---

## ✅ Keep Going

Red teaming AI is a moving target. Models change. Filters update.  
But the core skills—reasoning, creativity, persistence—_stay valuable_.

> Practice often. Share ethically.  
> Help others learn to break AI… so we can make it safer.
